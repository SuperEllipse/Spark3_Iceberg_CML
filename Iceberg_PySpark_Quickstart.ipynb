{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "private-diversity",
   "metadata": {},
   "source": [
    "# Using Apache Iceberg with Spark 3 in CML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-silly",
   "metadata": {},
   "source": [
    "The official documentation for Apache Iceberg with Spark is located at [this link](https://iceberg.apache.org/#getting-started/#using-iceberg-in-spark-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-burns",
   "metadata": {},
   "source": [
    "For a full list of Apache Iceberg terms, please visit [this link](https://iceberg.apache.org/#terms/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3071c73f-9846-435b-847a-f6799335b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP TABLE IF EXISTS spark_catalog.testdb.newtesttable\")\n",
    "#spark.sql(\"DROP TABLE IF EXISTS spark_catalog.testdb.secondtesttable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-committee",
   "metadata": {},
   "source": [
    "### Start a PySpark Session as shown below. You will want to set the Spark Catalog configurations as shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incomplete-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]') \\\n",
    "  .config(\"spark.jars.packages\",\"org.apache.iceberg:iceberg-spark3-runtime:0.12.1\") \\\n",
    "  .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "  .config(\"spark.sql.catalog.spark_catalog.type\",\"hive\") \\\n",
    "  .getOrCreate()\"\"\"\n",
    "\n",
    "\"\"\"SimpleApp.py\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "  .appName(\"1.1 - Ingest\") \\\n",
    "  .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\", \"us-east-2\")\\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\", \"s3a://demo-aws-go02\")\\\n",
    "  .config(\"spark.jars\",\"/home/cdsw/lib/iceberg-spark3-runtime-0.9.1.1.13.317211.0-9.jar\") \\\n",
    "  .config(\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "  .config(\"spark.sql.catalog.spark_catalog.type\",\"hive\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-level",
   "metadata": {},
   "source": [
    "### Iceberg comes with catalogs that enable SQL commands to manage tables and load them by name. \n",
    "### Catalogs are configured using properties under spark.sql.catalog.(catalog_name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prompt-universe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|   testdb|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  # Using a local Spark Catalog\n",
    "\n",
    "#spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_catalog.newjar\")\n",
    "spark.sql(\"USE spark_catalog.testdb\")\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()\n",
    "#spark.sql(\"DROP TABLE testtable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-immune",
   "metadata": {},
   "source": [
    "### You can use simple Spark SQL commands to create Spark tables as you always have. Just make sure to specify the USING iceberg clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extended-stopping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS newtesttable (id bigint, data string) USING iceberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-pickup",
   "metadata": {},
   "source": [
    "### To select a specific table snapshot or the snapshot at some time, Iceberg supports two Spark read options:\n",
    "\n",
    "* snapshot-id selects a specific table snapshot\n",
    "* as-of-timestamp selects the current snapshot at a timestamp, in milliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-definition",
   "metadata": {},
   "source": [
    "#### You can view all snapshots associated with the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ec587b-605f-4a1c-9d42-292d02ed1b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, data: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM spark_catalog.testdb.newtesttable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "future-dividend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "|committed_at|snapshot_id|parent_id|operation|manifest_list|summary|\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"spark_catalog.testdb.newtesttable.snapshots\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-place",
   "metadata": {},
   "source": [
    "#### Or a full table version history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adequate-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+---------+-------------------+\n",
      "|made_current_at|snapshot_id|parent_id|is_current_ancestor|\n",
      "+---------------+-----------+---------+-------------------+\n",
      "+---------------+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"spark_catalog.testdb.newtesttable.history\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-active",
   "metadata": {},
   "source": [
    "#### To show a table’s data files and each file’s metadata, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "excessive-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+\n",
      "|content|file_path|file_format|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|\n",
      "+-------+---------+-----------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+\n",
      "+-------+---------+-----------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"spark_catalog.testdb.newtesttable.files\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-brooklyn",
   "metadata": {},
   "source": [
    "### A manifest file is a metadata file that lists a subset of data files that make up a snapshot.\n",
    "\n",
    "### Each data file in a manifest is stored with a partition tuple, column-level stats, and summary information used to prune splits during scan planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-participant",
   "metadata": {},
   "source": [
    "#### To show a table’s file manifests and each file’s metadata, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "solid-container",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+-------------------+\n",
      "|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|partition_summaries|\n",
      "+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+-------------------+\n",
      "+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"spark_catalog.testdb.newtesttable.manifests\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-smith",
   "metadata": {},
   "source": [
    "## Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-laser",
   "metadata": {},
   "source": [
    "### Using snapshots as shown above, we can insert some data into the table and roll back to its original state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "present-venezuela",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert using Iceberg format\n",
    "spark.sql(\"INSERT INTO spark_catalog.testdb.newtesttable VALUES (1, 'x'), (2, 'y'), (3, 'z')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faced-praise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   x|\n",
      "|  2|   y|\n",
      "|  3|   z|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query using select\n",
    "spark.sql(\"SELECT * FROM spark_catalog.testdb.newtesttable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "useful-belgium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   x|\n",
      "|  2|   y|\n",
      "|  3|   z|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query using DF - All Data\n",
    "df = spark.table(\"spark_catalog.testdb.newtesttable\")\n",
    "df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "subtle-samuel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp = 1652920085.489827\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "timestamp = datetime.timestamp(now)\n",
    "print(\"timestamp =\", timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-commerce",
   "metadata": {},
   "source": [
    "#### Timestamps can be tricky. Please make sure to round your timestamp as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "expanded-neighbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   x|\n",
      "|  2|   y|\n",
      "|  3|   z|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query using a point in time\n",
    "df = spark.read.option(\"as-of-timestamp\", int(timestamp*1000)).format(\"iceberg\").load(\"spark_catalog.testdb.newtesttable\")\n",
    "df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "parallel-ethnic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert using Iceberg format\n",
    "spark.sql(\"INSERT INTO spark_catalog.testdb.newtesttable VALUES (1, 'd'), (2, 'e'), (3, 'f')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c67832e-4a4c-45f3-b49b-b05bdad33996",
   "metadata": {},
   "source": [
    "### Let's insert more data into the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3293abf-bb3e-46bd-a993-0a0d10642238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert using Iceberg format\n",
    "import string\n",
    "import random\n",
    "\n",
    "for i in range(25):\n",
    "    number = random.randint(0, 10)\n",
    "    letter = random.choice(string.ascii_letters)\n",
    "    spark.sql(\"INSERT INTO spark_catalog.testdb.newtesttable VALUES ({}, '{}')\".format(number, letter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e10c40-d142-4858-85af-4d61996eb3cf",
   "metadata": {},
   "source": [
    "### Now let's access the data again. Let's access it with the same timestemp as before. Notice we have a smaller number of rows than we just inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dd0858e-93cf-4bbc-97d1-1055cb72bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   x|\n",
      "|  2|   y|\n",
      "|  3|   z|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query using a point in time\n",
    "df = spark.read.option(\"as-of-timestamp\", int(timestamp*1000)).format(\"iceberg\").load(\"spark_catalog.testdb.newtesttable\")\n",
    "df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbabc8-ef2e-4fdf-84cd-3cbf40b3391c",
   "metadata": {},
   "source": [
    "### Observe that many new Snapshots have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d35f7a32-7e20-41cd-9793-a963100eeac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2022-05-19 00:27:36.444|5797822168349485877|null               |true               |\n",
      "|2022-05-19 00:28:16.897|3326689113563627160|5797822168349485877|true               |\n",
      "|2022-05-19 00:36:41.604|2080039523045202901|3326689113563627160|true               |\n",
      "|2022-05-19 00:36:42.87 |8726800525946754873|2080039523045202901|true               |\n",
      "|2022-05-19 00:36:43.934|5250560437891035012|8726800525946754873|true               |\n",
      "|2022-05-19 00:36:44.966|3502850421386520639|5250560437891035012|true               |\n",
      "|2022-05-19 00:36:46.076|6484488418513084975|3502850421386520639|true               |\n",
      "|2022-05-19 00:36:47.109|6131843803655349628|6484488418513084975|true               |\n",
      "|2022-05-19 00:36:48.125|9149412625213001646|6131843803655349628|true               |\n",
      "|2022-05-19 00:36:49.154|7899258739660324236|9149412625213001646|true               |\n",
      "|2022-05-19 00:36:50.275|3534193066800053318|7899258739660324236|true               |\n",
      "|2022-05-19 00:36:51.352|1934686370749576161|3534193066800053318|true               |\n",
      "|2022-05-19 00:36:52.394|5426517119305366496|1934686370749576161|true               |\n",
      "|2022-05-19 00:36:53.462|373037863119265479 |5426517119305366496|true               |\n",
      "|2022-05-19 00:36:54.518|5758957638750915040|373037863119265479 |true               |\n",
      "|2022-05-19 00:36:55.545|8456174629502915114|5758957638750915040|true               |\n",
      "|2022-05-19 00:36:56.526|919494437088828330 |8456174629502915114|true               |\n",
      "|2022-05-19 00:36:57.604|8941097323185820492|919494437088828330 |true               |\n",
      "|2022-05-19 00:36:58.628|6624958993800383470|8941097323185820492|true               |\n",
      "|2022-05-19 00:36:59.632|8215457020905462961|6624958993800383470|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"spark_catalog.testdb.newtesttable.history\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97793e-b86b-4c42-8b85-68397bc44d18",
   "metadata": {},
   "source": [
    "### You can also query the table in its previous state as of a specific partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938fd327-6b8a-4bae-8449-1383daaf1bdc",
   "metadata": {},
   "source": [
    "#### Copy paste a partition_id from above and paste it in the next Spark command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e5621db-65e2-47c4-a3a9-e3422b780c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  8|   p|\n",
      "|  1|   d|\n",
      "|  2|   e|\n",
      "|  3|   f|\n",
      "|  1|   w|\n",
      "|  7|   v|\n",
      "|  0|   g|\n",
      "|  1|   x|\n",
      "|  2|   y|\n",
      "|  3|   z|\n",
      "|  7|   v|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read\\\n",
    "    .option(\"snapshot-id\", 6484488418513084975)\\\n",
    "    .table(\"spark_catalog.testdb.newtesttable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d0f6b-59a5-41f2-ba9b-2e1e8aa0d352",
   "metadata": {},
   "source": [
    "### The Iceberg API allows you to create tables from Spark Datafrmaes, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0efe36e2-b1cb-47ef-bed4-f902c1e232d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.sql(\"SELECT * FROM spark_catalog.testdb.newtesttable\").sample(fraction=0.5, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a3b301e-dfa7-445e-af5a-0d421ad32a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "| 10|   M|\n",
      "|  6|   u|\n",
      "|  3|   g|\n",
      "|  6|   F|\n",
      "|  8|   Q|\n",
      "|  2|   U|\n",
      "|  5|   p|\n",
      "|  7|   v|\n",
      "|  3|   X|\n",
      "|  8|   y|\n",
      "| 10|   w|\n",
      "|  3|   z|\n",
      "|  1|   x|\n",
      "|  2|   y|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56beefc7-74e6-4225-8f7d-96f8a891490e",
   "metadata": {},
   "source": [
    "#### Creating a new Spark Table with the API and Loading It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64a36034-a441-4f77-946a-ad182c325a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.writeTo(\"spark_catalog.testdb.secondtesttable\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7720ee5-a386-4ffb-ade1-0f30201a0f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO spark_catalog.testdb.secondtesttable SELECT * FROM spark_catalog.testdb.newtesttable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920de66-77da-4010-a632-e9dabc31bc24",
   "metadata": {},
   "source": [
    "#### The Python script iceberg.py shows a merge operation if you are interested"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
